{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes algorithm estimates the probability of a class (C) given a set of features (X) using the following formula:\n",
    "\n",
    "P(C | X) = ( P(X | C) * P(C) ) / P(X)\n",
    "\n",
    "Where:\n",
    "- P(C | X) is the posterior probability of class C given features X.\n",
    "- P(X | C) is the likelihood of observing features X given class C, assuming Gaussian distribution.\n",
    "- P(C) is the prior probability of class C.\n",
    "- P(X) is the probability of observing features X.\n",
    "\n",
    "In Gaussian Naive Bayes, the likelihood P(X | C) is modeled as a Gaussian (normal) distribution for each feature given each class.\n",
    "\n",
    "Assuming X = (x₁, x₂, ..., xₙ) represents the features and μᵢ and σᵢ are the mean and standard deviation of the i-th feature for class C, the Gaussian probability density function (PDF) for each feature is:\n",
    "\n",
    "P(xᵢ | C) = (1 / √(σᵢ * 2π)) * e^(-((xᵢ - μᵢ)²) / (2 * σᵢ²))\n",
    "\n",
    "Where:\n",
    "- xᵢ is the value of the i-th feature in the input.\n",
    "- μᵢ is the mean of the i-th feature for class C.\n",
    "- σᵢ is the standard deviation of the i-th feature for class C.\n",
    "- e is the base of the natural logarithm.\n",
    "- π is the mathematical constant pi.\n",
    "\n",
    "To classify an input with features X into a class C, you calculate the posterior probabilities for each class and choose the class with the highest probability.\n",
    "\n",
    "Keep in mind that Gaussian Naive Bayes assumes that features are independent given the class, which might not always hold in real-world scenarios. Despite this simplifying assumption, Gaussian Naive Bayes can work well for certain types of data distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = np.zeros((len(self.classes), n_features))\n",
    "        self.variances = np.zeros((len(self.classes), n_features))\n",
    "        self.priors = np.zeros(len(self.classes))\n",
    "\n",
    "        # Calculate mean, variance and prior (P(C)) for each class\n",
    "        for class_idx, c in enumerate(self.classes):\n",
    "            X_for_c = X[y == c]\n",
    "            self.means[class_idx] = X_for_c.mean(axis=0)\n",
    "            self.variances[class_idx] = X_for_c.var(axis=0)\n",
    "            self.priors[class_idx] = X_for_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._classify_sample(x) for x in X]\n",
    "    \n",
    "    def _classify_sample(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # Go through list of classes\n",
    "        for class_idx, c in enumerate(self.classes):\n",
    "            # Compute prior P(C) and likelihood P(x|C) = P(f1|C) * P(f2|C) * ... * P(fn|C)\n",
    "            # we use log so that we can add instead of multiply inorder to prevent underflow\n",
    "            prior = np.log(self.priors[class_idx])\n",
    "            likelihood = np.sum(np.log(self._calculate_likelihood(x, class_idx)))\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def _calculate_likelihood(self, x, class_idx):\n",
    "        mean = self.means[class_idx]\n",
    "        var = self.variances[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
