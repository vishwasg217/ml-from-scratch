{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = np.zeros((2, 5))\n",
    "zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes algorithm estimates the probability of a class (C) given a set of features (X) using the following formula:\n",
    "\n",
    "P(C | X) = ( P(X | C) * P(C) ) / P(X)\n",
    "\n",
    "Where:\n",
    "- P(C | X) is the posterior probability of class C given features X.\n",
    "- P(X | C) is the likelihood of observing features X given class C, assuming Gaussian distribution.\n",
    "- P(C) is the prior probability of class C.\n",
    "- P(X) is the probability of observing features X.\n",
    "\n",
    "In Gaussian Naive Bayes, the likelihood P(X | C) is modeled as a Gaussian (normal) distribution for each feature given each class.\n",
    "\n",
    "Assuming X = (x₁, x₂, ..., xₙ) represents the features and μᵢ and σᵢ are the mean and standard deviation of the i-th feature for class C, the Gaussian probability density function (PDF) for each feature is:\n",
    "\n",
    "P(xᵢ | C) = (1 / (σᵢ * √(2π))) * e^(-((xᵢ - μᵢ)²) / (2 * σᵢ²))\n",
    "\n",
    "Where:\n",
    "- xᵢ is the value of the i-th feature in the input.\n",
    "- μᵢ is the mean of the i-th feature for class C.\n",
    "- σᵢ is the standard deviation of the i-th feature for class C.\n",
    "- e is the base of the natural logarithm.\n",
    "- π is the mathematical constant pi.\n",
    "\n",
    "To classify an input with features X into a class C, you calculate the posterior probabilities for each class and choose the class with the highest probability.\n",
    "\n",
    "Keep in mind that Gaussian Naive Bayes assumes that features are independent given the class, which might not always hold in real-world scenarios. Despite this simplifying assumption, Gaussian Naive Bayes can work well for certain types of data distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = np.zeros(len(self.classes), n_features)\n",
    "        self.variances = np.zeros(len(self.classes), n_features)\n",
    "        self.priors = np.zeros(len(self.classes))\n",
    "\n",
    "        for class_idx, c in enumerate(self.classes):\n",
    "            X_for_c = X[y == c]\n",
    "            self.means[class_idx] = X_for_c.mean(axis=0)\n",
    "            self.variances[class_idx] = X_for_c.var(axis=0)\n",
    "            self.priors[class_idx] = X_for_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._classify_sample(x) for x in X]\n",
    "    \n",
    "    def _classify_sample(self, x):\n",
    "        posteriors = []\n",
    "        for class_idx, c in enumerate(self.classes):\n",
    "            prior = self.priors[class_idx]\n",
    "            likelihood = self._calculate_likelihood(x, class_idx)\n",
    "            posterior = prior * likelihood\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.84333333, 3.05733333, 3.758     , 1.19933333])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "np.argmax(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
