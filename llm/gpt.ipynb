{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "  BY\n",
      "\n",
      "  L. FRANK BAUM\n",
      "\n",
      "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ\n",
      "Chracters:  ['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "Vocab Size 81\n"
     ]
    }
   ],
   "source": [
    "with open('../data/wizard_of_oz.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:100])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Chracters: \",chars)\n",
    "print(\"Vocab Size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 62, 62, 1, 73, 61, 58, 71, 58]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text: str):\n",
    "    ans = []\n",
    "    for t in text:\n",
    "        ans.append(chars.index(t))\n",
    "    return ans\n",
    "\n",
    "encoded_hello = encode(\"hii there\")\n",
    "encoded_hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hii there'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(indices: list):\n",
    "    ans = \"\"\n",
    "    for i in indices:\n",
    "        ans += chars[i]\n",
    "    return ans\n",
    "\n",
    "decoded_hello = decode(encoded_hello)\n",
    "decoded_hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([232309]) torch.int64\n",
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.8)\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      "torch.Size([4, 8])\n",
      "tensor([[58,  1, 57, 68,  1, 67, 68, 76],\n",
      "        [72,  1, 55, 58, 62, 67, 60,  0],\n",
      "        [65, 58, 72, 11,  3,  0,  0,  3],\n",
      "        [ 0,  0,  3, 49, 68, 74, 71,  1]])\n",
      "y: \n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 57, 68,  1, 67, 68, 76, 24],\n",
      "        [ 1, 55, 58, 62, 67, 60,  0, 65],\n",
      "        [58, 72, 11,  3,  0,  0,  3, 47],\n",
      "        [ 0,  3, 49, 68, 74, 71,  1, 42]])\n",
      "Context: [58] | Target: 1\n",
      "Context: [58, 1] | Target: 57\n",
      "Context: [58, 1, 57] | Target: 68\n",
      "Context: [58, 1, 57, 68] | Target: 1\n",
      "Context: [58, 1, 57, 68, 1] | Target: 67\n",
      "Context: [58, 1, 57, 68, 1, 67] | Target: 68\n",
      "Context: [58, 1, 57, 68, 1, 67, 68] | Target: 76\n",
      "Context: [58, 1, 57, 68, 1, 67, 68, 76] | Target: 24\n",
      "Context: [72] | Target: 1\n",
      "Context: [72, 1] | Target: 55\n",
      "Context: [72, 1, 55] | Target: 58\n",
      "Context: [72, 1, 55, 58] | Target: 62\n",
      "Context: [72, 1, 55, 58, 62] | Target: 67\n",
      "Context: [72, 1, 55, 58, 62, 67] | Target: 60\n",
      "Context: [72, 1, 55, 58, 62, 67, 60] | Target: 0\n",
      "Context: [72, 1, 55, 58, 62, 67, 60, 0] | Target: 65\n",
      "Context: [65] | Target: 58\n",
      "Context: [65, 58] | Target: 72\n",
      "Context: [65, 58, 72] | Target: 11\n",
      "Context: [65, 58, 72, 11] | Target: 3\n",
      "Context: [65, 58, 72, 11, 3] | Target: 0\n",
      "Context: [65, 58, 72, 11, 3, 0] | Target: 0\n",
      "Context: [65, 58, 72, 11, 3, 0, 0] | Target: 3\n",
      "Context: [65, 58, 72, 11, 3, 0, 0, 3] | Target: 47\n",
      "Context: [0] | Target: 0\n",
      "Context: [0, 0] | Target: 3\n",
      "Context: [0, 0, 3] | Target: 49\n",
      "Context: [0, 0, 3, 49] | Target: 68\n",
      "Context: [0, 0, 3, 49, 68] | Target: 74\n",
      "Context: [0, 0, 3, 49, 68, 74] | Target: 71\n",
      "Context: [0, 0, 3, 49, 68, 74, 71] | Target: 1\n",
      "Context: [0, 0, 3, 49, 68, 74, 71, 1] | Target: 42\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 4  # number of sequences in a batch\n",
    "block_size = 8  # length of a particular sequence\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\" \n",
    "    Generate a small batch of data of inputs and targets\n",
    "    \"\"\"\n",
    "\n",
    "    data = train_data if split == \"train_data\" else test_data\n",
    "\n",
    "    # generates batch_size number of indexes from until the length of data - block size\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(\"X: \")\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "print('y: ')\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for l in range(block_size):\n",
    "        context = x[b, :l+1]\n",
    "        target = y[b, l]\n",
    "        print(f\"Context: {context.tolist()} | Target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "----\n",
      "B: \n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----\n",
      "C: \n",
      " tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# We use tril to replicate the above pattern, where each row of triangle is a sequence in a batch.\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print(f\"A: \\n {a}\\n----\")\n",
    "print(f\"B: \\n {b}\\n----\")\n",
    "print(f\"C: \\n {c}\\n----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tril enables us to create a lower triangular matrix from a given matrix. \n",
    "\n",
    "This is useful summing up the context in given sequence.\n",
    "\n",
    "In the above example B is a sequence. When multiplied with A, we get a new matrix C, which has the summation of the of the context of each word in the until that particular index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing we can also perform average of the given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "----\n",
      "B: \n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----\n",
      "C: \n",
      " tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "a = a / a.sum(1, keepdim=True)\n",
    "c = a @ b\n",
    "\n",
    "print(f\"A: \\n {a}\\n----\")\n",
    "print(f\"B: \\n {b}\\n----\")\n",
    "print(f\"C: \\n {c}\\n----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3332, -1.1723, -1.0216, -0.0545, -1.0950,  0.2735,  0.1340, -0.8490],\n",
      "        [-0.6597,  0.7869, -1.2725,  1.6851,  0.1159,  0.5450,  0.2356, -0.1962],\n",
      "        [ 0.3630, -1.5219,  0.7821, -1.7215, -0.3494,  0.2884, -0.1021, -1.4271],\n",
      "        [-0.1001,  0.8649, -0.0335,  1.0221, -0.1350, -0.3078,  0.1440, -0.3019],\n",
      "        [ 0.0136, -1.6202, -1.9888, -0.3327, -1.2506, -0.8928, -2.2674,  3.0561],\n",
      "        [-0.5833,  1.2025, -0.3281,  0.9147,  0.9809, -0.4859,  1.7589,  0.1650],\n",
      "        [ 1.1351, -1.9940,  1.5545, -1.8037, -0.5062, -2.6109, -1.0739,  1.6430],\n",
      "        [-1.2784, -0.4554, -1.4118,  0.6392, -0.5780,  1.9291,  1.6689,  0.1103]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Post masking: \n",
      "torch.Size([4, 8, 8])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1905, 0.8095, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3742, 0.0568, 0.5690, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1288, 0.3380, 0.1376, 0.3956, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4311, 0.0841, 0.0582, 0.3049, 0.1217, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0537, 0.3205, 0.0694, 0.2404, 0.2568, 0.0592, 0.0000, 0.0000],\n",
      "        [0.3396, 0.0149, 0.5165, 0.0180, 0.0658, 0.0080, 0.0373, 0.0000],\n",
      "        [0.0165, 0.0375, 0.0144, 0.1120, 0.0332, 0.4069, 0.3136, 0.0660]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pandas import value_counts\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # Shape: (B,T,16)\n",
    "q = query(x) # Shape: (B,T,16)\n",
    "w = q @ k.transpose(-2, -1) # Shape: (B,T,16) @ (B,16,T) --> (B,T,T)(4, 8, 8). Transposing necessary for matmul.\n",
    "\n",
    "print(w[0])\n",
    "# masking the weights is necessary to ensure that future time step isnt being taken into consideration.\n",
    "# Hence the upper triangle is being masked with -inf\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = w.masked_fill(tril == 0, float('-inf'))\n",
    "w = F.softmax(w, dim=-1)\n",
    "print(\"Post masking: \")\n",
    "print(w.shape)\n",
    "print(w[0])\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weights (w): Tell us how much each word (or time step) should focus on every other word when generating its new representation.\n",
    "\n",
    "- Value (v): Represents the actual content or features of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1905, 0.8095, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3742, 0.0568, 0.5690, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1288, 0.3380, 0.1376, 0.3956, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4311, 0.0841, 0.0582, 0.3049, 0.1217, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0537, 0.3205, 0.0694, 0.2404, 0.2568, 0.0592, 0.0000, 0.0000],\n",
       "        [0.3396, 0.0149, 0.5165, 0.0180, 0.0658, 0.0080, 0.0373, 0.0000],\n",
       "        [0.0165, 0.0375, 0.0144, 0.1120, 0.0332, 0.4069, 0.3136, 0.0660]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x)\n",
    "out = w @ v\n",
    "\n",
    "out.shape\n",
    "w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the tensor:\n",
    "\n",
    "\n",
    "- The tensor represents represents one sequence within a batch of sequences.  \n",
    "- Each row corresponds to a time step (or word, in the context of our previous example).\n",
    "- Each column in a given row represents the attention score of the word at that time step with every other word in the sequence, including itself.\n",
    "- The values are between 0 and 1 (due to the softmax operation) and represent the attention probabilities.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The first row `[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]` indicates that the first word is only attending to itself and not to any other word in the sequence.\n",
    "  \n",
    "- The second row `[0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]` indicates that the second word is mostly attending to itself (with a score of 0.8426) but also has some attention (0.1574) on the first word. It doesn't attend to any word after it, which is consistent with the masking we applied to prevent attending to future words.\n",
    "\n",
    "\n",
    "\n",
    "The last row of the tensor is:\n",
    "`[0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]`\n",
    "\n",
    "This row represents the attention scores of the last word (or time step) in the sequence with respect to every other word in the sequence, including itself. Here's what we can infer from these scores:\n",
    "\n",
    "1. **Self-Attention**: The last word has an attention score of `0.2391` with itself. This means that when generating the output representation for this word, about 23.91% of the information will come from the word itself.\n",
    "\n",
    "2. **Previous Words**: The last word is attending to all the previous words in the sequence, but the attention is distributed unevenly:\n",
    "   - The word gives the highest attention to the 7th word with a score of `0.2423`.\n",
    "   - The next highest attention is given to the 4th word with a score of `0.2297`.\n",
    "   - The rest of the words receive relatively lower attention, with scores ranging from `0.0210` to `0.0843`.\n",
    "\n",
    "3. **No Future Words**: As expected (due to the masking), the last word doesn't attend to any \"future\" words since it's the last word in the sequence.\n",
    "\n",
    "4. **Interpretation**: The last word in the sequence seems to be most influenced by the 7th and 4th words when determining its output representation. This could mean that, in the context of the data, the 7th and 4th words have the most relevant information or context for the last word.\n",
    "\n",
    "- As we go down the rows, we can see that each word can attend to itself and any previous word in the sequence, but not to words that come after it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, embedding_dim, block_size, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # we use scaling factor C**0.5 to enure that the dot product doesn't become too large\n",
    "        w = (q @ k.transpose(-2, -1)) / (C**0.5)\n",
    "        w = w.masked_fill(self.tril == 0, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = w @ v\n",
    "        return out\n",
    "\n",
    "# head_size = 16\n",
    "# embedding_dim = 32\n",
    "# block_size = 8\n",
    "# head = Head(head_size=head_size, embedding_dim=embedding_dim, block_size=block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n",
      "tensor([[-1.2264, -0.0980, -0.8409,  0.7529,  0.7038,  0.2454, -0.7274,  1.0205,\n",
      "         -0.3150, -0.0000,  0.0290,  0.4161, -0.5935, -1.3211,  1.2606, -0.1903,\n",
      "          0.9041,  0.9761, -0.0000, -0.2970, -0.0188,  0.1085,  0.8519, -0.4045,\n",
      "         -0.0579, -0.5410,  0.4703,  1.1554, -0.0844,  1.1649, -0.8749, -0.2854],\n",
      "        [ 0.4522, -0.8784, -1.0006,  0.3476,  0.0000,  0.6824,  0.0044,  0.0000,\n",
      "         -0.3643, -1.1138, -0.2615,  0.9210, -0.3555,  0.0619, -0.7586, -0.0000,\n",
      "          0.3884,  0.4608,  0.3313, -0.0000, -0.1791,  0.2638,  0.5144, -1.5159,\n",
      "         -0.1547, -0.2613, -0.8164, -0.5472, -0.0000,  0.4320,  0.8985,  0.0686],\n",
      "        [-0.0232,  0.0763, -0.2703,  0.4221,  0.1750, -0.7853,  0.4311, -0.3606,\n",
      "          0.5848,  0.4205, -0.0000,  0.8547,  0.4291,  0.2485,  0.7969, -0.2059,\n",
      "          0.0000, -0.0120,  0.4014,  0.6077, -0.2883, -0.2684, -0.1635, -0.1079,\n",
      "          0.0209, -0.5439,  0.9277, -0.6792, -0.6998, -0.0581, -0.3495,  1.0145],\n",
      "        [-0.7159, -0.0678, -0.5483,  0.2106, -0.0000, -0.0062,  0.8537, -0.6249,\n",
      "          0.7748,  0.1206, -0.0000,  0.4824, -0.4832, -0.1176,  0.0172, -0.4786,\n",
      "         -0.0913,  0.1948,  0.0814,  0.0751, -0.0624,  0.2225, -0.2815,  0.0877,\n",
      "          0.2139, -0.0038, -0.2394, -0.0000,  0.0107,  0.2767, -0.2493,  0.1059],\n",
      "        [-0.0575,  0.1842,  1.4990, -0.1737, -0.6462, -0.0182,  0.1339, -0.8274,\n",
      "         -0.8033, -0.2586, -1.4834, -0.7944,  0.5263, -0.2179,  0.9758, -0.3654,\n",
      "          0.4189, -0.1435,  0.7446, -0.9207,  0.9610,  1.1191,  0.7494,  0.0209,\n",
      "          0.2555,  0.5807,  0.0055, -0.3792,  1.2507, -0.6474,  0.0821,  0.8029],\n",
      "        [-0.1560,  0.5940, -0.7563, -0.8729,  0.6055, -0.7886, -0.1918,  1.5967,\n",
      "         -0.6066, -0.0431,  0.9415, -0.9692, -0.2178, -0.4475, -0.9445, -0.6554,\n",
      "         -0.4386, -1.1684, -1.3365, -0.2473,  0.6030,  0.9123,  0.6229,  0.1705,\n",
      "         -0.0392,  0.1524, -0.0000,  0.0000,  0.6852, -0.3208, -0.3190, -0.8804],\n",
      "        [ 0.9105, -0.0088,  0.4697,  1.1761,  0.0000,  0.4250, -0.3183,  0.4596,\n",
      "          0.3765, -1.4454, -1.0595, -0.5249, -0.5541, -0.5889,  0.4427,  0.9844,\n",
      "         -0.2706, -1.5269,  1.0145, -0.9670,  0.2120, -0.8217,  0.3111, -0.3877,\n",
      "          0.8199,  0.1345,  0.8644,  0.5113,  0.3117, -0.7972,  0.2826,  1.2449],\n",
      "        [ 0.9356, -0.3554, -0.0000,  0.7710,  0.0943, -0.4977,  0.3036,  1.1988,\n",
      "         -0.2487,  0.7264, -0.5088,  0.4843,  0.7724, -1.0390,  0.3341, -0.1244,\n",
      "         -0.3485, -0.5537,  0.1219,  0.6332,  0.4045, -0.5886,  0.1012, -0.8521,\n",
      "         -0.0875,  0.0737, -0.9051, -0.8640, -0.8181,  1.2498, -0.9117,  0.5301]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, embedding_dim, block_size, dropout=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(embedding_dim//num_heads, embedding_dim, block_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [head(x) for head in self.heads]\n",
    "        out = torch.concat(out, dim=-1)\n",
    "        out = self.dropout(self.proj(x))\n",
    "        return out\n",
    "    \n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "num_heads = 8\n",
    "\n",
    "x = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "multi_head = MultiHeadAttention(num_heads, embedding_dim, block_size)\n",
    "output = multi_head(x)\n",
    "print(output.shape)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "from torch import dropout\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffnn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, embedding_dim, block_size, dropout=0.05):\n",
    "        self.attention = MultiHeadAttention(num_heads, embedding_dim, block_size)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.ffnn = FeedForward(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.norm1(self.attention(x) + x)\n",
    "        out = self.norm2(self.ffnn(x) + x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
